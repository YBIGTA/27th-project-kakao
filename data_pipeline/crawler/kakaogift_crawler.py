# -*- coding: utf-8 -*-
import os
import re
import time
from urllib.parse import urljoin

import pandas as pd
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException, WebDriverException
from webdriver_manager.chrome import ChromeDriverManager

from base_crawler import BaseCrawler


class KakaoGiftCrawler(BaseCrawler):
    """
    (ì˜ˆì™¸ ì „ë¶€ ì œì™¸) ì¼ë°˜ ì¹´í…Œê³ ë¦¬ ì „ìš© í¬ë¡¤ëŸ¬
    - ìƒìœ„ ì¹´í…Œê³ ë¦¬: ì§€ì • ëª©ë¡ ì¤‘ 'ì™€ì¸/ì–‘ì£¼/ì „í†µì£¼', 'ì•„í‹°ìŠ¤íŠ¸/ìºë¦­í„°'ëŠ” ê±´ë„ˆëœ€
    - íë¦„: ìƒìœ„ ì¹´í…Œê³ ë¦¬ â†’ í•˜ìœ„ ì¹´í…Œê³ ë¦¬ â†’ (ìƒí’ˆ íƒ­ í™œì„±í™”) â†’ ì¹´ë“œ ëª©ë¡ ìˆ˜ì§‘
    - ì¹´ë“œ í•„ë“œ: brand, product_name, price, satisfaction_pct, review_count,
                wish_count, tags, product_url
    """

    BASE = "https://gift.kakao.com"
    DEFAULT_START = "https://gift.kakao.com/home?categoryLayer=OPEN"

    # ë™ì‘ íŒŒë¼ë¯¸í„°
    SCROLL_PAUSE = 0.8
    CLICK_PAUSE = 0.5
    MAX_SCROLL_TRIES = 50  # 300ê°œê°€ ëª¨ì¼ ë•Œê¹Œì§€ ì¶©ë¶„íˆ ìŠ¤í¬ë¡¤
    IMPLICIT_WAIT = 5

    # ìƒìœ„ ì¹´í…Œê³ ë¦¬ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸(ìš”ì²­ ëª©ë¡)
    TOP_WHITELIST = [
        "êµí™˜ê¶Œ", "ìƒí’ˆê¶Œ", "ë·°í‹°", "íŒ¨ì…˜", "ì‹í’ˆ",
        "ì™€ì¸/ì–‘ì£¼/ì „í†µì£¼", "ë¦¬ë¹™/ë„ì„œ", "ë ˆì €/ìŠ¤í¬ì¸ ",
        "ì•„í‹°ìŠ¤íŠ¸/ìºë¦­í„°", "ìœ ì•„ë™/ë°˜ë ¤", "ë””ì§€í„¸/ê°€ì „", "ì¹´ì¹´ì˜¤í”„ë Œì¦ˆ",
    ]
    # ì´ë²ˆ ë¼ìš´ë“œì—ì„œëŠ” ì „ë¶€ ì œì™¸
    TOP_EXCLUDE_FOR_NOW = {"ì™€ì¸/ì–‘ì£¼/ì „í†µì£¼", "ì•„í‹°ìŠ¤íŠ¸/ìºë¦­í„°"}

    # ì¼ë°˜ì ìœ¼ë¡œ ìˆ¨ê¸¸ ì„œë¸Œì¹´í…Œê³ ë¦¬
    EXCLUDE_SUBCATS = {
        "ìš”ì¦˜ ë·°í‹°", "í¬ì¸íŠ¸ì ë¦½ ë¸Œëœë“œ", "ì‹ ê·œì…ì  ë¸Œëœë“œ", "MDì¶”ì²œ", "ì‹ ê·œì…ì ",
        "ì¶”ì²œ", "ê³„ì ˆê°€ì „", "ì „ì²´", "ì™€ì¸/ì–‘ì£¼/ë§¥ì£¼"
    }

    def __init__(
        self,
        output_dir: str,
        *,
        output_filename: str = "kakao_gifts.csv",
        headless: bool = False,
        items_per_subcat: int = 300,
        start_url: str | None = None,
        top_filter: str | None = None,
    ):
        super().__init__(output_dir=output_dir)
        self.output_path = os.path.join(output_dir, output_filename)

        self.headless = headless
        self.items_per_subcat = items_per_subcat
        self.start_url = start_url or self.DEFAULT_START
        self.top_filter = top_filter

        self.driver = None
        self.all_rows = []

    # -------------------- 1) ë¸Œë¼ìš°ì € --------------------
    def start_browser(self):
        opts = Options()
        if self.headless:
            opts.add_argument("--headless=new")
        opts.add_argument("--window-size=1500,1100")
        opts.add_argument("--no-sandbox")
        opts.add_argument("--disable-gpu")
        opts.add_argument("--lang=ko-KR")
        opts.add_argument("--disable-dev-shm-usage")
        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=opts)
        self.driver.implicitly_wait(self.IMPLICIT_WAIT)

    # -------------------- ìœ í‹¸ --------------------
    def wait(self, t=None):
        time.sleep(t if t is not None else self.SCROLL_PAUSE)

    def js_click(self, el):
        try:
            self.driver.execute_script("arguments[0].click();", el)
        except WebDriverException:
            try:
                el.click()
            except Exception:
                pass
        self.wait(self.CLICK_PAUSE)

    @staticmethod
    def safe_text(el):
        try:
            return el.text.strip()
        except Exception:
            return ""

    @staticmethod
    def to_int(text):
        if not text:
            return None
        s = re.sub(r"[^\dë§Œ\+,\s]", "", text).replace(",", "").replace(" ", "")
        if "ë§Œ+" in s:
            try:
                return int(s.replace("ë§Œ+", "")) * 10000
            except Exception:
                return None
        if "ë§Œ" in s:
            try:
                return int(s.replace("ë§Œ", "")) * 10000
            except Exception:
                pass
        try:
            return int(re.sub(r"[^\d]", "", s))
        except Exception:
            return None

    # -------------------- ì¹´í…Œê³ ë¦¬ íŒ¨ë„ / í—¤ë” ë²„íŠ¼ --------------------
    def category_panel_open(self) -> bool:
        """ì¹´í…Œê³ ë¦¬ íŒ¨ë„(ì¢Œì¸¡ ë ˆì´ì–´)ì´ ì—´ë ¤ ìˆëŠ”ì§€ í™•ì¸"""
        try:
            self.driver.find_element(By.CSS_SELECTOR, ".category_layer .list_ctgmenu")
            return True
        except NoSuchElementException:
            return False

    def click_header_category_button(self) -> bool:
        """
        í—¤ë”ì˜ 'ì¹´í…Œê³ ë¦¬' ë²„íŠ¼ì„ ëˆŒëŸ¬ íŒ¨ë„ì„ ì—°ë‹¤.
        ë‹¤ì–‘í•œ ë§ˆí¬ì—… ì¼€ì´ìŠ¤ë¥¼ ëŒ€ë¹„í•´ ì—¬ëŸ¬ ì…€ë ‰í„°ë¥¼ ì‹œë„.
        """
        selectors = [
            "button.btn_cate",
            "div.group_head button.btn_cate",
            "button[aria-label='ì¹´í…Œê³ ë¦¬']",
            "a[aria-label='ì¹´í…Œê³ ë¦¬']",
            "a[href*='categoryLayer']",
        ]
        for sel in selectors:
            try:
                btn = self.driver.find_element(By.CSS_SELECTOR, sel)
                self.js_click(btn)
                self.wait(0.6)
                if self.category_panel_open():
                    return True
            except (NoSuchElementException, StaleElementReferenceException):
                continue
        return False

    def ensure_category_panel(self):
        """
        ì¹´í…Œê³ ë¦¬ íŒ¨ë„ì„ ë°˜ë“œì‹œ ì—°ë‹¤.
        - í—¤ë” ë²„íŠ¼ í´ë¦­ ì‹œë„
        - ê·¸ë˜ë„ ì‹¤íŒ¨í•˜ë©´ OPEN íŒŒë¼ë¯¸í„° í˜ì´ì§€ë¡œ ê°•ë³µê·€
        """
        if self.category_panel_open():
            return
        if not self.click_header_category_button():
            self.driver.get(self.DEFAULT_START)
            self.wait(1.0)

    # -------------------- ì¹´í…Œê³ ë¦¬/íƒ­ í•¸ë“¤ëŸ¬ --------------------
    def list_top_categories(self):
        els = self.driver.find_elements(
            By.CSS_SELECTOR,
            ".category_layer .list_ctgmenu a.link_menu span.txt_menu",
        )
        return [self.safe_text(e) for e in els if self.safe_text(e)]

    def click_top_category(self, name):
        els = self.driver.find_elements(
            By.CSS_SELECTOR, ".category_layer .list_ctgmenu a.link_menu"
        )
        for a in els:
            txt = self.safe_text(a.find_element(By.CSS_SELECTOR, "span.txt_menu"))
            if txt == name:
                self.js_click(a)
                self.wait(0.8)
                return True
        return False

    def list_sub_categories(self):
        els = self.driver.find_elements(
            By.CSS_SELECTOR, ".category_layer .list_ctgsub a.link_menu span.txt_menu"
        )
        return [self.safe_text(e) for e in els if self.safe_text(e)]

    def click_sub_category(self, name):
        els = self.driver.find_elements(
            By.CSS_SELECTOR, ".category_layer .list_ctgsub a.link_menu"
        )
        for a in els:
            txt = self.safe_text(a.find_element(By.CSS_SELECTOR, "span.txt_menu"))
            if txt == name:
                self.js_click(a)
                self.wait(1.0)
                return True
        return False

    # -------------------- í˜ì´ì§€ ë‚´ í•˜ìœ„ ì¹´í…Œê³ ë¦¬ ë²„íŠ¼ ì œì–´ --------------------
    def list_page_sub_categories(self):
        """í˜ì´ì§€ ë‚´ í•˜ìœ„ ì¹´í…Œê³ ë¦¬ ë²„íŠ¼ë“¤ì„ ì°¾ì•„ì„œ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
        els = self.driver.find_elements(
            By.CSS_SELECTOR, 
            ".list_ctgmain li a.link_ctg, .wrap_depth1 a.link_ctg, .area_ctglist a.link_ctg"
        )
        subcats = []
        for el in els:
            try:
                # data-tiara-copy ì†ì„±ì—ì„œ ì¹´í…Œê³ ë¦¬ëª… ê°€ì ¸ì˜¤ê¸°
                text = el.get_attribute("data-tiara-copy")
                if not text:
                    # ì§ì ‘ í…ìŠ¤íŠ¸ì—ì„œ ê°€ì ¸ì˜¤ê¸°
                    text = self.safe_text(el)
                if text and text not in subcats:
                    subcats.append(text)
            except Exception:
                continue
        return subcats

    def click_page_sub_category(self, name):
        """í˜ì´ì§€ ë‚´ í•˜ìœ„ ì¹´í…Œê³ ë¦¬ ë²„íŠ¼ í´ë¦­"""
        els = self.driver.find_elements(
            By.CSS_SELECTOR, 
            ".list_ctgmain li a.link_ctg, .wrap_depth1 a.link_ctg, .area_ctglist a.link_ctg"
        )
        for el in els:
            try:
                # data-tiara-copy ì†ì„± í™•ì¸
                text = el.get_attribute("data-tiara-copy")
                if not text:
                    # ì§ì ‘ í…ìŠ¤íŠ¸ í™•ì¸
                    text = self.safe_text(el)
                if text == name:
                    self.js_click(el)
                    self.wait(1.0)
                    return True
            except Exception:
                continue
        return False

    def scroll_to_top(self):
        """í˜ì´ì§€ ë§¨ ìœ„ë¡œ ìŠ¤í¬ë¡¤"""
        self.driver.execute_script("window.scrollTo(0, 0);")
        self.wait(0.5)

    # --- í˜ì´ì§€ ë‚´ 'ìƒí’ˆ/ë¸Œëœë“œ' íƒ­ ì œì–´ ---
    def ensure_product_tab(self):
        """
        ì¼ë°˜ ì¹´í…Œê³ ë¦¬ì—ì„œ 'ìƒí’ˆ' íƒ­ì„ í™œì„±í™”í•œë‹¤.
        """
        candidate_a = self.driver.find_elements(
            By.CSS_SELECTOR,
            ".wrap_srchtab a.link_tab, "
            ".module_wrapper .module_tab .rail_cate a.link_tab, "
            ".rail_cate a.link_tab"
        )
        for a in candidate_a:
            try:
                label = self.safe_text(
                    a.find_element(By.CSS_SELECTOR, "span.txt_tab, span.txt_g, span")
                )
            except NoSuchElementException:
                label = self.safe_text(a)
            if label == "ìƒí’ˆ":
                self.js_click(a)
                return True
        spans = self.driver.find_elements(
            By.CSS_SELECTOR, "span.txt_tab, span.txt_g, .tablist span"
        )
        for s in spans:
            if self.safe_text(s) == "ìƒí’ˆ":
                self.js_click(s)
                return True
        return False

    # -------------------- ìŠ¤í¬ë¡¤/ì¹´ë“œ íŒŒì‹± --------------------
    def scroll_until_cards(self, min_cards, list_selector="ul.list_prd > li"):
        print(f"        ì¹´ë“œ ë¡œë”© ëŒ€ê¸° ì¤‘...")
        for i in range(6):
            cards = self.driver.find_elements(By.CSS_SELECTOR, list_selector)
            if len(cards) > 0:
                print(f"        âœ“ ì¹´ë“œ ë°œê²¬: {len(cards)}ê°œ")
                break
            print(f"        ëŒ€ê¸° {i+1}/6...")
            self.wait(0.6)

        last_len, still = 0, 0
        tries = 0
        while True:
            cards = self.driver.find_elements(By.CSS_SELECTOR, list_selector)
            if len(cards) >= min_cards:
                print(f"        ìŠ¤í¬ë¡¤ ì™„ë£Œ: {len(cards)}ê°œ (ëª©í‘œ: {min_cards}ê°œ ë‹¬ì„±)")
                break
            if tries >= self.MAX_SCROLL_TRIES:
                print(f"        ìŠ¤í¬ë¡¤ ì™„ë£Œ: {len(cards)}ê°œ (ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ë„ë‹¬)")
                break
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            self.wait(self.SCROLL_PAUSE)
            cards2 = self.driver.find_elements(By.CSS_SELECTOR, list_selector)
            still = still + 1 if len(cards2) == last_len else 0
            last_len = len(cards2)
            tries += 1
            if tries % 3 == 0:
                print(f"        ìŠ¤í¬ë¡¤ {tries}: {len(cards2)}ê°œ (ëª©í‘œ: {min_cards}ê°œ)")
            if still >= 4:
                print(f"        ë” ì´ìƒ ìƒˆë¡œìš´ ì¹´ë“œê°€ ë¡œë“œë˜ì§€ ì•ŠìŒ")
                break

    def parse_card(self, card):
        data = {
            "brand": None,
            "product_name": None,
            "price": None,
            "satisfaction_pct": None,
            "review_count": None,
            "wish_count": None,
            "tags": None,
            "product_url": None,
        }
        # ë§í¬
        for sel in ["div.thumb_prd a.link_thumb[href]", "gc-link a.link_info[href]"]:
            try:
                href = card.find_element(By.CSS_SELECTOR, sel).get_attribute("href")
                if href:
                    data["product_url"] = urljoin(self.BASE, href)
                    break
            except NoSuchElementException:
                pass
        # í…ìŠ¤íŠ¸ë“¤
        try:
            data["brand"] = self.safe_text(card.find_element(By.CSS_SELECTOR, "span.brand_prd"))
        except NoSuchElementException:
            pass
        try:
            data["product_name"] = self.safe_text(card.find_element(By.CSS_SELECTOR, "strong.txt_prdname"))
        except NoSuchElementException:
            pass
        for sel in ["em.num_price", "span.price_info em.num_price"]:
            try:
                data["price"] = self.to_int(self.safe_text(card.find_element(By.CSS_SELECTOR, sel)))
                if data["price"] is not None:
                    break
            except NoSuchElementException:
                pass
        # ë§Œì¡±ë„/ë¦¬ë·°
        roots = [card]
        try:
            roots.insert(0, card.find_element(By.CSS_SELECTOR, "div.info_prd"))
        except NoSuchElementException:
            pass
        for r in roots:
            if data["satisfaction_pct"] is None:
                try:
                    s = self.safe_text(r.find_element(By.CSS_SELECTOR, "span.txt_star"))
                    m = re.search(r"(\d+)\s*%", s)
                    data["satisfaction_pct"] = int(m.group(1)) if m else None
                except NoSuchElementException:
                    pass
            if data["review_count"] is None:
                try:
                    rv = self.safe_text(r.find_element(By.CSS_SELECTOR, "span.txt_review"))
                    data["review_count"] = self.to_int(rv)
                except NoSuchElementException:
                    pass
        # ìœ„ì‹œ/íƒœê·¸
        try:
            data["wish_count"] = self.to_int(self.safe_text(card.find_element(By.CSS_SELECTOR, "span.num_wsh")))
        except NoSuchElementException:
            pass
        try:
            tags = [self.safe_text(t) for t in card.find_elements(By.CSS_SELECTOR, "span.tag_info span.tag_g")]
            data["tags"] = "|".join([t for t in tags if t]) if tags else None
        except NoSuchElementException:
            pass
        return data

    def parse_card_from_html(self, soup):
        """BeautifulSoupìœ¼ë¡œ HTML ìŠ¤ëƒ…ìƒ·ì—ì„œ ì¹´ë“œ ì •ë³´ íŒŒì‹±"""
        data = {
            "brand": None,
            "product_name": None,
            "price": None,
            "satisfaction_pct": None,
            "review_count": None,
            "wish_count": None,
            "tags": None,
            "product_url": None,
        }
        
        # ë§í¬
        for sel in ["div.thumb_prd a.link_thumb[href]", "gc-link a.link_info[href]"]:
            link_elem = soup.select_one(sel)
            if link_elem and link_elem.get("href"):
                data["product_url"] = urljoin(self.BASE, link_elem.get("href"))
                break
        
        # í…ìŠ¤íŠ¸ë“¤
        brand_elem = soup.select_one("span.brand_prd")
        if brand_elem:
            data["brand"] = brand_elem.get_text(strip=True)
        
        name_elem = soup.select_one("strong.txt_prdname")
        if name_elem:
            data["product_name"] = name_elem.get_text(strip=True)
        
        # ê°€ê²©
        for sel in ["em.num_price", "span.price_info em.num_price"]:
            price_elem = soup.select_one(sel)
            if price_elem:
                price_text = price_elem.get_text(strip=True)
                data["price"] = self.to_int(price_text)
                if data["price"] is not None:
                    break
        
        # ë§Œì¡±ë„/ë¦¬ë·°
        info_prd = soup.select_one("div.info_prd")
        roots = [soup]
        if info_prd:
            roots.insert(0, info_prd)
        
        for root in roots:
            if data["satisfaction_pct"] is None:
                star_elem = root.select_one("span.txt_star")
                if star_elem:
                    star_text = star_elem.get_text(strip=True)
                    m = re.search(r"(\d+)\s*%", star_text)
                    data["satisfaction_pct"] = int(m.group(1)) if m else None
            
            if data["review_count"] is None:
                review_elem = root.select_one("span.txt_review")
                if review_elem:
                    review_text = review_elem.get_text(strip=True)
                    data["review_count"] = self.to_int(review_text)
        
        # ìœ„ì‹œ/íƒœê·¸
        wish_elem = soup.select_one("span.num_wsh")
        if wish_elem:
            data["wish_count"] = self.to_int(wish_elem.get_text(strip=True))
        
        tag_elems = soup.select("span.tag_info span.tag_g")
        if tag_elems:
            tags = [tag.get_text(strip=True) for tag in tag_elems if tag.get_text(strip=True)]
            data["tags"] = "|".join(tags) if tags else None
        
        return data

    def collect_current_page(self, want_total, meta, seen_urls, already_collected=0):
        print(f"      ìŠ¤í¬ë¡¤ ì‹œì‘ (ëª©í‘œ: {want_total}ê°œ)...")
        self.scroll_until_cards(max(already_collected, want_total))
        
        # ìƒí’ˆ ì¹´ë“œ ì°¾ê¸°
        cards = self.driver.find_elements(By.CSS_SELECTOR, "ul.list_prd > li")
        print(f"      ë°œê²¬ëœ ìƒí’ˆ ì¹´ë“œ: {len(cards)}ê°œ")
        
        if len(cards) == 0:
            # ë‹¤ë¥¸ ì„ íƒì ì‹œë„
            alternative_selectors = [
                ".list_prd li",
                ".list_product li", 
                ".product_list li",
                ".item_list li",
                "[class*='product'] li",
                "[class*='item'] li"
            ]
            for selector in alternative_selectors:
                cards = self.driver.find_elements(By.CSS_SELECTOR, selector)
                if len(cards) > 0:
                    print(f"      ëŒ€ì²´ ì„ íƒì '{selector}'ë¡œ {len(cards)}ê°œ ë°œê²¬")
                    break
        
        # ìŠ¤ëƒ…ìƒ· íŒŒì‹±ìœ¼ë¡œ stale ê·¼ë³¸ ì°¨ë‹¨
        print(f"      HTML ìŠ¤ëƒ…ìƒ· ìˆ˜ì§‘ ì¤‘...")
        html_snaps = []
        for i, c in enumerate(cards):
            try:
                html_snaps.append(c.get_attribute("outerHTML"))
            except Exception as e:
                print(f"        ! ì¹´ë“œ {i} HTML ì¶”ì¶œ ì‹¤íŒ¨: {str(e)[:30]}...")
                continue
        
        print(f"      ìŠ¤ëƒ…ìƒ· {len(html_snaps)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ, íŒŒì‹± ì‹œì‘...")
        
        rows, new_count = [], 0
        for i, html in enumerate(html_snaps):
            try:
                soup = BeautifulSoup(html, "html.parser")
                item = self.parse_card_from_html(soup)
                url = item.get("product_url")
                if not url or url in seen_urls:
                    continue
                rows.append({**item, **meta})
                seen_urls.add(url)
                new_count += 1
                if already_collected + new_count >= want_total:
                    break
                if new_count % 10 == 0:
                    print(f"        ì§„í–‰: {new_count}ê°œ ìˆ˜ì§‘ë¨")
            except Exception as e:
                print(f"        ! ìŠ¤ëƒ…ìƒ· {i} íŒŒì‹± ì‹¤íŒ¨: {str(e)[:50]}...")
                continue
        
        print(f"      ìµœì¢… ìˆ˜ì§‘: {new_count}ê°œ")
        return rows, new_count

    # -------------------- ìˆ˜ì§‘ê¸°(ì¼ë°˜ë§Œ) --------------------
    def crawl_general(self, top, sub, n):
        """ì¼ë°˜: (ìƒí’ˆ íƒ­ìœ¼ë¡œ ì „í™˜í•œ ë’¤) ì¹´ë“œì—ì„œ nê°œ ëª©í‘œ(ë¶€ì¡±í•˜ë©´ ìˆëŠ” ë§Œí¼)"""
        print(f"[{top} > {sub}] ì¼ë°˜ ìˆ˜ì§‘(ìƒí’ˆ íƒ­)")
        
        # ìƒí’ˆ íƒ­ í™œì„±í™” ì‹œë„
        print(f"    ìƒí’ˆ íƒ­ í™œì„±í™” ì‹œë„...")
        if self.ensure_product_tab():
            print(f"    âœ“ ìƒí’ˆ íƒ­ í™œì„±í™” ì„±ê³µ")
        else:
            print(f"    ! ìƒí’ˆ íƒ­ í™œì„±í™” ì‹¤íŒ¨, í˜„ì¬ íƒ­ìœ¼ë¡œ ì§„í–‰")
        
        # í˜„ì¬ í˜ì´ì§€ URL í™•ì¸
        current_url = self.driver.current_url
        print(f"    í˜„ì¬ URL: {current_url}")
        
        # ìƒí’ˆ ì¹´ë“œ ìˆ˜ì§‘
        print(f"    ìƒí’ˆ ì¹´ë“œ ìˆ˜ì§‘ ì‹œì‘ (ëª©í‘œ: {n}ê°œ)...")
        rows, collected = self.collect_current_page(
            n,
            {"top_category": top, "sub_category": sub, "sub_tab": None},
            set(),
            0,
        )
        print(f"    âœ“ ìˆ˜ì§‘ ì™„ë£Œ: {collected}ê°œ")
        return rows

    # -------------------- 2) í¬ë¡¤ë§ íŒŒì´í”„ë¼ì¸ --------------------
    def scrape_data(self):
        self.driver.get(self.start_url)
        self.wait(1.2)
        self.ensure_category_panel()
        self.all_rows = []

        avail_tops = self.list_top_categories()
        # ìš”ì²­í•œ 12ê°œ ì¤‘ í˜„ì¬ ë¼ìš´ë“œì—ì„œ ì œì™¸í•  2ê°œ ì œê±°
        avail_tops = [t for t in avail_tops if t in self.TOP_WHITELIST and t not in self.TOP_EXCLUDE_FOR_NOW]
        if self.top_filter:
            allow = [t.strip() for t in self.top_filter.split(",")]
            target_tops = [t for t in avail_tops if t in allow]
        else:
            target_tops = avail_tops

        print("ëŒ€ìƒ ìƒìœ„ ì¹´í…Œê³ ë¦¬(ì˜ˆì™¸ ì œì™¸):", target_tops)

        for top in target_tops:
            print(f"\n=== ìƒìœ„ ì¹´í…Œê³ ë¦¬ ì‹œì‘: {top} ===")
            
            # ë§¤ ìƒìœ„ ì¹´í…Œê³ ë¦¬ ì§„ì… ì‹œ í•­ìƒ OPEN í˜ì´ì§€ë¡œ ë¦¬ì…‹
            self.driver.get(self.DEFAULT_START)
            self.wait(1.2)
            
            # íŒ¨ë„ ì—´ê¸° ë³´ì¥
            self.ensure_category_panel()
            print(f"  ìƒìœ„ ì¹´í…Œê³ ë¦¬ '{top}' í´ë¦­ ì‹œë„...")
            if not self.click_top_category(top):
                print(f"  ! ìƒìœ„ ì¹´í…Œê³ ë¦¬ í´ë¦­ ì‹¤íŒ¨: {top}")
                continue
            print(f"  âœ“ ìƒìœ„ ì¹´í…Œê³ ë¦¬ '{top}' í´ë¦­ ì„±ê³µ")

            # íŒ¨ë„ì—ì„œ í•˜ìœ„ ì¹´í…Œê³ ë¦¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            panel_subcats = self.list_sub_categories()
            if not panel_subcats:
                print(f"  ! í•˜ìœ„ ì¹´í…Œê³ ë¦¬ ì—†ìŒ: {top}")
                continue

            # ì²« ë²ˆì§¸ í•˜ìœ„ ì¹´í…Œê³ ë¦¬ë¡œ ì´ë™ (íŒ¨ë„ì—ì„œ í´ë¦­)
            first_sub = None
            for sub in panel_subcats:
                if sub not in self.EXCLUDE_SUBCATS:
                    first_sub = sub
                    break
            
            if not first_sub:
                print(f"  ! ìœ íš¨í•œ í•˜ìœ„ ì¹´í…Œê³ ë¦¬ ì—†ìŒ: {top}")
                continue

            if not self.click_sub_category(first_sub):
                print(f"    ! ì²« ë²ˆì§¸ í•˜ìœ„ ì¹´í…Œê³ ë¦¬ í´ë¦­ ì‹¤íŒ¨: {first_sub}")
                continue

            # ì²« ë²ˆì§¸ í•˜ìœ„ ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘
            try:
                rows = self.crawl_general(top, first_sub, self.items_per_subcat)
                self.all_rows.extend(rows)
                print(f"    âœ“ {first_sub}: {len(rows)}ê°œ ìˆ˜ì§‘")
                # ì²« ë²ˆì§¸ í•˜ìœ„ ì¹´í…Œê³ ë¦¬ ì¦‰ì‹œ ì €ì¥
                if rows:
                    self.save_to_database(rows, append_mode=True)
                    print(f"    ğŸ’¾ {first_sub} ì €ì¥ ì™„ë£Œ")
            except Exception as e:
                print(f"    ! {first_sub} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

            # í˜ì´ì§€ ë‚´ í•˜ìœ„ ì¹´í…Œê³ ë¦¬ ë²„íŠ¼ë“¤ë¡œ ë‚˜ë¨¸ì§€ ìˆœíšŒ
            page_subcats = self.list_page_sub_categories()
            print(f"    í˜ì´ì§€ ë‚´ í•˜ìœ„ ì¹´í…Œê³ ë¦¬ ë°œê²¬: {len(page_subcats)}ê°œ")
            
            for sub in page_subcats:
                if sub in self.EXCLUDE_SUBCATS or sub == first_sub:
                    continue
                
                # í˜ì´ì§€ ìœ„ë¡œ ìŠ¤í¬ë¡¤
                self.scroll_to_top()
                self.wait(0.5)
                
                # í˜ì´ì§€ ë‚´ ë²„íŠ¼ í´ë¦­
                if not self.click_page_sub_category(sub):
                    print(f"      ! í˜ì´ì§€ ë‚´ í•˜ìœ„ ì¹´í…Œê³ ë¦¬ í´ë¦­ ì‹¤íŒ¨: {sub}")
                    continue

                # ìˆ˜ì§‘
                try:
                    rows = self.crawl_general(top, sub, self.items_per_subcat)
                    self.all_rows.extend(rows)
                    print(f"      âœ“ {sub}: {len(rows)}ê°œ ìˆ˜ì§‘")
                    # ê° í•˜ìœ„ ì¹´í…Œê³ ë¦¬ ì¦‰ì‹œ ì €ì¥
                    if rows:
                        self.save_to_database(rows, append_mode=True)
                        print(f"      ğŸ’¾ {sub} ì €ì¥ ì™„ë£Œ")
                except Exception as e:
                    print(f"      ! {sub} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

            print(f"\n=== {top} ì¹´í…Œê³ ë¦¬ ì™„ë£Œ: {len([r for r in self.all_rows if r.get('top_category') == top])}ê°œ ìˆ˜ì§‘ ===")
            print(f"=== {top} ì¹´í…Œê³ ë¦¬ ì €ì¥ ì™„ë£Œ ===\n")

        return self.all_rows

    # -------------------- 3) ì €ì¥ --------------------
    def save_to_database(self, data, append_mode=False):
        df = pd.DataFrame(data)
        if df.empty:
            print("ìˆ˜ì§‘ ê²°ê³¼ê°€ ë¹„ì—ˆìŠµë‹ˆë‹¤.")
            return
        
        df.drop_duplicates(subset=["product_url"], inplace=True)
        cols = [
            "top_category", "sub_category", "sub_tab",
            "brand", "product_name", "price",
            "satisfaction_pct", "review_count", "wish_count",
            "tags", "product_url",
        ]
        df = df[[c for c in cols if c in df.columns]]
        
        if append_mode and os.path.exists(self.output_path):
            # ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ ì¶”ê°€ ëª¨ë“œë¡œ ì €ì¥
            df.to_csv(self.output_path, mode='a', header=False, index=False, encoding="utf-8-sig")
            print(f"ì¶”ê°€ ì €ì¥ ì™„ë£Œ: {self.output_path} (+{len(df)}ê°œ)")
        else:
            # ìƒˆ íŒŒì¼ë¡œ ì €ì¥
            df.to_csv(self.output_path, index=False, encoding="utf-8-sig")
            print(f"ì €ì¥ ì™„ë£Œ: {self.output_path} (ì´ {len(df)}ê°œ)")

    # -------------------- íŒŒì´í”„ë¼ì¸ --------------------
    def run(self):
        self.start_browser()
        try:
            data = self.scrape_data()
            # ë§ˆì§€ë§‰ì— ì „ì²´ ë°ì´í„°ë„ ì €ì¥ (ì¤‘ë³µ ì œê±° í¬í•¨)
            if data:
                print(f"\n=== ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ: {len(data)}ê°œ ===")
                self.save_to_database(data, append_mode=False)
        finally:
            self.close_browser()
